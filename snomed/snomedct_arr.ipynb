{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352f7fc-ba04-4c54-ad07-f7895c2d3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "from owlready2.pymedtermino2 import *\n",
    "from owlready2.pymedtermino2.umls import *\n",
    "default_world.set_backend(filename = \"pym.sqlite3\")\n",
    "# import_umls(\"umls-2019AA-metathesaurus.zip\", terminologies = [\"ICD10\", \"SNOMEDCT_US\", \"CUI\"])\n",
    "PYM = get_ontology(\"http://PYM/\").load()\n",
    "SNOMEDCT_US = PYM[\"SNOMEDCT_US\"]\n",
    "GROUP_SIZE = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020b4b9-c9d6-4331-8833-da0cca9b9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordhoard import Synonyms\n",
    "from wordhoard import Antonyms\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3837339-c876-4789-9be8-1700b4c718e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = PYM[\"MESH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56929123-92b7-4918-ad78-a6955f26970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ICD10 = PYM[\"ICD10\"]\n",
    "CUI = PYM[\"CUI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a34c9c4-a91e-4622-8bf3-c6cb5a3727c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"all_list_of_dicts.tsv\"\n",
    "list_of_dicts = []\n",
    "lines = []\n",
    "with open(file) as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "count = 0\n",
    "for d in lines:\n",
    "    count+=1\n",
    "    curr_dict = eval(d.strip())\n",
    "    list_of_dicts.append(curr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b49ed-08c4-480f-b1ab-dbbc22a945ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cui_to_snomed_id = {} # one to many mapping\n",
    "phrase_to_cui = {} # one to many mapping\n",
    "with open(\"snomed_all_phrases_cui.tsv\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines[1:]:\n",
    "        phrase,cui,synonyms,snomed_id = [x.strip() for x in line.split(\"\\t\")]\n",
    "        cui = eval(cui)\n",
    "        for c in cui:  \n",
    "            if c in cui_to_snomed_id:\n",
    "                cui_to_snomed_id[c].add(snomed_id)\n",
    "            else:\n",
    "                cui_to_snomed_id[c] = {snomed_id}\n",
    "            if phrase.lower() in phrase_to_cui:\n",
    "                phrase_to_cui[phrase.lower()].add(c)\n",
    "            else:\n",
    "                phrase_to_cui[phrase.lower()] = {c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ca6ce-4656-423a-85cf-be56e3ef1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list_of_dicts = []\n",
    "key_count = {'prefLabel':0, 'synonym':0, 'cui':0, 'semanticType':0, 'children':0, 'parents':0, \n",
    "         'descendants':0, 'ancestors':0}\n",
    "for d in list_of_dicts:\n",
    "    num_keys = 0\n",
    "    for k in d.keys():\n",
    "        if k in key_count:\n",
    "            num_keys+=1\n",
    "    if num_keys == len(key_count):\n",
    "        at_least_one = False\n",
    "        for c in d['cui']:\n",
    "            if c in cui_to_snomed_id:\n",
    "                at_least_one=True\n",
    "        if at_least_one:\n",
    "            filtered_list_of_dicts.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78073c5-5b3e-4654-a800-6e4392a8c005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write filtered_list_of_dict\n",
    "with open(\"filtered_list_of_dicts.tsv\",\"w\") as fw:\n",
    "    for d in filtered_list_of_dicts:\n",
    "        fw.write(f\"{d}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f20ca3-b582-4a8f-a9c6-a199d82d7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = 0\n",
    "lengths2 = 0\n",
    "for i, d in enumerate(filtered_list_of_dicts):\n",
    "    all_sids = []\n",
    "    all_inters = []\n",
    "    all_interprets = []\n",
    "    all_mapped = []\n",
    "    for c in d['cui']:\n",
    "        sids = cui_to_snomed_id[c]\n",
    "        # all_sids.append(sid)\n",
    "        all_sids.extend(sids)\n",
    "        \n",
    "        # there may be multiple sids - i.e. Edentulous and Absence of teeth \n",
    "        # map to same cui \n",
    "        inter = []\n",
    "        interprets = []\n",
    "        mapped_to = []\n",
    "        \n",
    "        for sid in sids:\n",
    "            inter.extend(SNOMEDCT_US[int(sid)].has_interpretation)\n",
    "            interprets.extend(SNOMEDCT_US[int(sid)].interprets)\n",
    "            mapped_to.extend(SNOMEDCT_US[int(sid)].mapped_to)\n",
    "\n",
    "\n",
    "        for curr_inter in inter: all_inters.append(curr_inter.label[0])\n",
    "        for curr_interprets in interprets: all_interprets.append(curr_interprets.label[0])\n",
    "        for curr_mapped in mapped_to: all_mapped.append(curr_mapped.label[0])\n",
    "\n",
    "\n",
    "    filtered_list_of_dicts[i]['interpretations'] = all_inters\n",
    "    filtered_list_of_dicts[i]['interprets'] = all_interprets\n",
    "    filtered_list_of_dicts[i]['mappedTo'] = all_mapped\n",
    "    filtered_list_of_dicts[i]['sids'] = all_sids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf702d52-5ad1-4290-b1d9-ceac5c5a63e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_children = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6d93fe-9d07-4a63-9df1-50a431165047",
   "metadata": {},
   "outputs": [],
   "source": [
    "snomed_roots = {123037004,404684003,308916002,272379006,363787002,410607006,373873005,78621006,260787004,71388002,362981000,419891008,243796009,900000000000441003,48176007,370115009,123038009,254291000,105590001 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747bc40-8085-4105-a8b0-2e54ba611560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_tree(root,set_of_interest):\n",
    "    if root is None:\n",
    "        return\n",
    "    set_of_interest.add(root.name)\n",
    "    children = root.children\n",
    "    for child in root.children:\n",
    "        traverse_tree(child,set_of_interest)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251d4d4-b1c2-4286-a3d7-045614575867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse over the entire tree to get all nodes\n",
    "all_nodes = set()\n",
    "for curr_root in snomed_roots:\n",
    "    curr_root_concept = SNOMEDCT_US[curr_root]\n",
    "    traverse_tree(curr_root_concept,all_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479283aa-b7e7-4fd5-83a6-fc2381073409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_and_acronyms(field):\n",
    "    words = set()\n",
    "    acronyms = set()\n",
    "    with open(f\"field_words/{field}_words.tsv\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word = line.strip().lower()\n",
    "            if len(word)>1:\n",
    "                words.add(word)\n",
    "\n",
    "    if os.path.exists(f\"field_acronyms/{field}_acronyms.tsv\"):\n",
    "        with open(f\"field_acronyms/{field}_acronyms.tsv\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                word = line.strip().lower()\n",
    "                if len(word)>1:\n",
    "                    acronyms.add(word)\n",
    "                \n",
    "    return words, acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236210f-00e3-4292-a8dc-725002fd69ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: if you want find terms specific to subspecialties\n",
    "cardiology_words, cardiology_acronyms = get_words_and_acronyms(\"cardiology\")\n",
    "endicronology_words, endicronology_acronyms = get_words_and_acronyms(\"endocrinology\")\n",
    "immuno_words, immuno_acronyms = get_words_and_acronyms(\"immuno\")\n",
    "surgery_words, surgery_acronyms = get_words_and_acronyms(\"surgery\")\n",
    "gynecology_words, gynecology_acronyms = get_words_and_acronyms(\"female_reproductive\")\n",
    "obstetrics_words, obstetrics_acronyms = get_words_and_acronyms(\"obstetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ea289e-178b-4abf-9386-b0f96ae634ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_field_terms(words, acronyms):\n",
    "    # only care about nodes relating to cardio\n",
    "    term_count = 0\n",
    "    terms = set()\n",
    "    for node in all_nodes:\n",
    "        assert len(SNOMEDCT_US[node].label) == 1\n",
    "        curr_label = SNOMEDCT_US[node].label[0]\n",
    "        curr_label_words = curr_label.split(\" \")\n",
    "        for w in words:\n",
    "            for c in curr_label_words:\n",
    "                if w.lower() in c.lower():\n",
    "                    terms.add(node)\n",
    "                    \n",
    "        for w in acronyms:\n",
    "            for c in curr_label_words:\n",
    "                if w == c:\n",
    "                    terms.add(node)\n",
    "                    \n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08df493c-2d4a-43e3-8188-ae3f071a2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardiology_terms = get_field_terms(cardiology_words, cardiology_acronyms)\n",
    "endicronology_terms = get_field_terms(endicronology_words, endicronology_acronyms)\n",
    "immuno_terms = get_field_terms(immuno_words, immuno_acronyms)\n",
    "surgery_terms = get_field_terms(surgery_words, surgery_acronyms)\n",
    "gynecology_terms = get_field_terms(gynecology_words, gynecology_acronyms)\n",
    "obstetrics_terms = get_field_terms(obstetrics_words, obstetrics_acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db61ea55-d99e-4026-a375-7a2ad6749d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_cardio_set_of_children = set()\n",
    "for cardio_term in cardiology_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[cardio_term]\n",
    "    traverse_tree(curr_root_concept,all_cardio_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee58917-efc7-45fe-9996-f28cbed2ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_endocrinology_set_of_children = set()\n",
    "for endocrinology_term in endicronology_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[endocrinology_term]\n",
    "    traverse_tree(curr_root_concept,all_endocrinology_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ab98c-6bde-4f8e-9da5-fe132d5033fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_gynecology_set_of_children = set()\n",
    "for gynecology_term in gynecology_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[gynecology_term]\n",
    "    traverse_tree(curr_root_concept,all_gynecology_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d64c659-4332-458d-8de4-ee0f45ebd5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_surgery_set_of_children = set()\n",
    "for surgery_term in surgery_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[surgery_term]\n",
    "    traverse_tree(curr_root_concept,all_surgery_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0076eab-43cb-4d5c-a672-7524ffa8069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_immuno_set_of_children = set()\n",
    "for immuno_term in immuno_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[immuno_term]\n",
    "    traverse_tree(curr_root_concept,all_immuno_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead97c2-3e35-4b38-8b89-e8d36ca8e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_obstetrics_set_of_children = set()\n",
    "for obstetrics_term in obstetrics_terms:\n",
    "    curr_root_concept = SNOMEDCT_US[obstetrics_term]\n",
    "    traverse_tree(curr_root_concept,all_obstetrics_set_of_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceafe3b7-53f9-4780-a177-6b4b34bd9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we don't want just them, we also want all cardio node children\n",
    "all_cardio_set_of_children_prime = set()\n",
    "for cardio_term in cardio_terms_prime:\n",
    "    curr_root_concept = SNOMEDCT_US[cardio_term]\n",
    "    traverse_tree(curr_root_concept,all_cardio_set_of_children_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52794c83-6a8e-45de-a0b0-3e8a4252752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('synonym_dict.json', 'r') as file:\n",
    "    synonym_dict = json.load(file)\n",
    "    \n",
    "with open('antonym_dict.json', 'r') as file:\n",
    "    antonym_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce7283-9f16-4e8e-8968-7d0818dbe83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "useless_set = {'\\x1b', '[', '3', '8', ';', '2', ';', '2', '5', '5', ';', '0', ';', '2', '5', '5', 'm', 'N', 'o', ' ', 's', 'y', 'n', 'o', 'n', 'y', 'm', 's', ' ', 'w', 'e', 'r', 'e', ' ', 'f', 'o', 'u', 'n', 'd', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ':', ' ', 'h', 'y', 'p', 'o', 'k', 'i', 'n', 'e', 't', 'i', 'c', ' ', '\\n', 'P', 'l', 'e', 'a', 's', 'e', ' ', 'v', 'e', 'r', 'i', 'f', 'y', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'd', ' ', 'i', 's', ' ', 's', 'p', 'e', 'l', 'l', 'e', 'd', ' ', 'c', 'o', 'r', 'r', 'e', 'c', 't', 'l', 'y', '.', ' ', '\\x1b', '[', '3', '8', ';', '2', ';', '2', '5', '5', ';', '2', '5', '5', ';', '2', '5', '5', 'm'}\n",
    "synonym_dict_updated = {}\n",
    "antonym_dict_updated = {}\n",
    "for word,values in synonym_dict.items():\n",
    "    new_values = set()\n",
    "    for val in values:\n",
    "        if val not in useless_set and len(val) > 1:\n",
    "            new_values.add(val.lower())\n",
    "    assert word not in synonym_dict_updated\n",
    "    synonym_dict_updated[word] = list(new_values)\n",
    "    \n",
    "for word,values in antonym_dict.items():\n",
    "    new_values = set()\n",
    "    for val in values:\n",
    "        if val not in useless_set and len(val) > 1:\n",
    "            new_values.add(val.lower())\n",
    "    assert word not in antonym_dict_updated\n",
    "    antonym_dict_updated[word] = list(new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb9b65-3b6d-483d-8ef2-8010fcfcc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_common_words(phrase_1_words, phrase_2_words):\n",
    "    intersection = set(phrase_1_words).intersection(set(phrase_2_words))\n",
    "    \n",
    "    uniq_phrase_1_words = []\n",
    "    uniq_phrase_2_words = []\n",
    "    \n",
    "    for word in phrase_1_words:\n",
    "        if word not in intersection:\n",
    "            uniq_phrase_1_words.append(word)\n",
    "            \n",
    "    for word in phrase_2_words:\n",
    "        if word not in intersection:\n",
    "            uniq_phrase_2_words.append(word) \n",
    "            \n",
    "    return uniq_phrase_1_words, uniq_phrase_2_words\n",
    "\n",
    "def find_intersection(phrase_1, phrase_2):\n",
    "    phrase_1, phrase_2 = phrase_1.lower(), phrase_2.lower()\n",
    "    phrase_1_words = phrase_1.split(\" \")\n",
    "    phrase_2_words = phrase_2.split(\" \")\n",
    "    \n",
    "    uniq_phrase_1_words, uniq_phrase_2_words = remove_common_words(phrase_1_words,phrase_2_words)\n",
    "    \n",
    "    return uniq_phrase_1_words, uniq_phrase_2_words\n",
    "\n",
    "def remove_stop_words(phrase):  \n",
    "    word_tokens = word_tokenize(phrase)\n",
    "\n",
    "    filtered_phrase = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    filtered_phrase = []\n",
    "    filtered_string = phrase \n",
    "    for i, w in enumerate(word_tokens):\n",
    "        if w not in stop_words:\n",
    "            filtered_phrase.append(w)\n",
    "        else:\n",
    "            filtered_string = filtered_string.replace(w+\" \",\"\")\n",
    "            filtered_string = filtered_string.replace(w,\"\")\n",
    "\n",
    "    return filtered_string\n",
    "\n",
    "\n",
    "def remove_stop_words_v2(phrase):  \n",
    "    word_tokens = phrase.split(\" \")\n",
    "    \n",
    "    stop_words_v2 = {\"in\", \"of\", \"or\", \"due\",\"to\", \"on\",\"at\"}\n",
    "\n",
    "    filtered_phrase = [w for w in word_tokens if not w.lower() in stop_words_v2]\n",
    "    filtered_phrase = []\n",
    "    filtered_string = phrase.lower() \n",
    "    for i, w in enumerate(word_tokens):\n",
    "        w = w.lower()\n",
    "        if w not in stop_words_v2:\n",
    "            filtered_phrase.append(w)\n",
    "        else:\n",
    "            filtered_string = filtered_string.replace(w+\" \",\"\")\n",
    "            filtered_string = filtered_string.replace(w,\"\")\n",
    "            \n",
    "    # remove ekg/ecg stuff\n",
    "    filtered_string=filtered_string.replace(\"ekg: \",\"\")\n",
    "    filtered_string=filtered_string.replace(\"ecg: \",\"\")\n",
    "    filtered_string=filtered_string.replace(\"ekg:\",\"\")\n",
    "    filtered_string=filtered_string.replace(\"ecg:\",\"\")\n",
    "    return filtered_string\n",
    "\n",
    "\n",
    "def take_care_of_special_cases(uniq_phrase_1_words, uniq_phrase_2_words, word1_l, word2_l, auto_label):\n",
    "    if \"|\" in word1_l or \"|\" in word2_l:\n",
    "        word1_l_arr = [x.strip() for x in word1_l.split(\"|\")]\n",
    "        word2_l_arr = [x.strip() for x in word2_l.split(\"|\")]\n",
    "        word1_l_set = set(word1_l_arr)\n",
    "        word2_l_set = set(word2_l_arr)\n",
    "\n",
    "        intersection = word1_l_set.intersection(word2_l_set)\n",
    "\n",
    "        # at least one of them completely matches.. not a contradiction by our heuristic\n",
    "        if len(intersection) == len(word1_l_set) or len(intersection) == len(word2_l_set):\n",
    "            auto_label = \"0\"\n",
    "        # both of them have extra information... most likely contradiction\n",
    "        else:\n",
    "            auto_label = \"1\"\n",
    "            \n",
    "    if (\"right\" in uniq_phrase_1_words and \"left\" in uniq_phrase_2_words) or (\"right\" in uniq_phrase_2_words and \"left\" in uniq_phrase_1_words):\n",
    "        auto_label=\"0\"\n",
    "    \n",
    "    return auto_label\n",
    "\n",
    "\n",
    "def use_synonym_heuristic(uniq_phrase_1_words,uniq_phrase_2_words,auto_label,phrase1,phrase2):\n",
    "    if len(uniq_phrase_1_words) == 1 and len(uniq_phrase_2_words) == 1:\n",
    "        word1 = uniq_phrase_1_words[0]\n",
    "        word2 = uniq_phrase_2_words[0]\n",
    "        is_synonym = False\n",
    "        is_antonym = False\n",
    "\n",
    "        # assert word1 in synonym_dict\n",
    "        if word1 not in synonym_dict:\n",
    "            synonym_dict[word1] = []\n",
    "            for syn in wn.synsets(word1):\n",
    "                for lemma in syn.lemma_names():\n",
    "                    synonym_dict[word1].append(lemma)\n",
    "                    if lemma == word2 and lemma != word1:\n",
    "                        is_synonym = True\n",
    "                        auto_label = \"0\"\n",
    "            for syn in Synonyms(word1).find_synonyms():\n",
    "                synonym_dict[word1].append(syn)\n",
    "                if syn == word2:\n",
    "                    is_synonym = True\n",
    "                    auto_label = \"0\"\n",
    "        else: # already have populated synonyms\n",
    "            for syn in synonym_dict[word1]:\n",
    "                if syn == word2:\n",
    "                    is_synonym = True\n",
    "                    auto_label = \"0\"\n",
    "\n",
    "        # assert word1 in antonym_dict\n",
    "        if word1 not in antonym_dict:\n",
    "            antonym_dict[word1] = []\n",
    "            for syn in wn.synsets(word1):\n",
    "                for i in syn.lemmas():\n",
    "                    if i.antonyms():\n",
    "                        ant = i.antonyms()[0].name()\n",
    "                        antonym_dict[word1].append(ant)\n",
    "                        if ant == word2:\n",
    "                            is_antonym = True\n",
    "            for ant in Antonyms(word1).find_antonyms():\n",
    "                antonym_dict[word1].append(ant)\n",
    "                if ant == word2:\n",
    "                    is_antonym = True\n",
    "                    auto_label = \"1\"\n",
    "        else: # already have populated antonyms\n",
    "            for ant in antonym_dict[word1]:\n",
    "                if ant == word2:\n",
    "                    is_antonym = True\n",
    "                    auto_label = \"1\"\n",
    "        \n",
    "    return auto_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc701718-676d-4dd7-9c51-1395f8ac23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_label(phrases, interpretations, use_synonyms=True, rid_of_stop_words=True, special_cases=True):\n",
    "    phrase1,phrase2 = phrases\n",
    "    interp1,interp2 = interpretations\n",
    "    \n",
    "    naive_intersection = interp1.intersection(interp2)\n",
    "    naive_label = \"1\" if len(naive_intersection) == 0 else \"0\"\n",
    "    \n",
    "    reconciled_label = naive_label\n",
    "    \n",
    "    if rid_of_stop_words:\n",
    "        phrase1 = remove_stop_words_v2(phrase1)\n",
    "        phrase2 = remove_stop_words_v2(phrase2)\n",
    "\n",
    "    uniq_phrase_1_words, uniq_phrase_2_words = find_intersection(phrase1,phrase2)\n",
    "\n",
    "    if len(uniq_phrase_1_words) == 1 and len(uniq_phrase_2_words) == 1:\n",
    "        set_of_unique_words.add(uniq_phrase_1_words[0])\n",
    "        set_of_unique_words.add(uniq_phrase_2_words[0])\n",
    "    \n",
    "    if use_synonyms:\n",
    "        reconciled_label = use_synonym_heuristic(uniq_phrase_1_words,uniq_phrase_2_words,reconciled_label,phrase1,phrase2)\n",
    "        \n",
    "    if special_cases:\n",
    "        reconciled_label = take_care_of_special_cases(uniq_phrase_1_words, uniq_phrase_2_words, interp1, interp2, reconciled_label)\n",
    "                 \n",
    "            \n",
    "    # IMPORTANT!: when reconciled claims its a contradiction, but naive claims it isnt... most of the time\n",
    "    # reconciled is correct (i.e. lots of types 'Abnormal')\n",
    "    # However... when reconciled claims non-contradiction, but naive_label claims contradiction... naive is\n",
    "    # correct most of the time.. this usually happens because of 'special case'.. maybe should just get rid of it\n",
    "    \n",
    "    if str(reconciled_label)=='0' and str(naive_label) =='1':\n",
    "        final_label = naive_label\n",
    "    elif str(reconciled_label)=='1' and str(naive_label) =='0':\n",
    "        final_label = reconciled_label\n",
    "    else: # they are the same\n",
    "        final_label = naive_label\n",
    "        \n",
    "    return final_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd209f1e-ca93-48b2-9814-5c628284a882",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_unique_words = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625bac11-4c8c-4cfe-9f23-1f416e9e7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the tree ancestry pattern of the given i_by's\n",
    "import math\n",
    "\n",
    "# only cardio focused...\n",
    "def clean_phrases_of_colon_prefixes(i_by):\n",
    "    phrase = i_by.label[0].lower()\n",
    "    phrase = phrase.replace(\"ekg: \",\"\")\n",
    "    phrase = phrase.replace(\"ecg: \",\"\")\n",
    "    i_by.label[0]=phrase\n",
    "    return i_by\n",
    "\n",
    "def clean_phrases_of_colon_prefixes_phrase(phrase):\n",
    "    phrase = phrase.lower()\n",
    "    phrase = phrase.replace(\"ekg: \",\"\")\n",
    "    phrase = phrase.replace(\"ecg: \",\"\")\n",
    "    return phrase\n",
    "    \n",
    "def determine_interpretation_relationship(interpreted_by, single_word_dict, phrase_dict, sample=True):\n",
    "    # {i_by: self and parents}\n",
    "    dict_interpreted_by = {}\n",
    "    count = 0\n",
    "    # i_by is a phrase which describes a certain observed outcome.. i.e. \n",
    "    # Increased vascular resistance\n",
    "    for i_by in interpreted_by:\n",
    "        # for the i_by example above.. i.e. increase (usually single word, but sometimes 2+)\n",
    "        has_interpretations = [x.label[0] for x in i_by.has_interpretation]\n",
    "        \n",
    "        # if any of the parents are the same between the i_by, then they are same category (of contra spectrum)\n",
    "        assert i_by not in dict_interpreted_by\n",
    "        modified_parents = []\n",
    "        \n",
    "        modified_parents = i_by.parents\n",
    "                \n",
    "        parents_and_self = modified_parents + [i_by] \n",
    "        dict_interpreted_by[i_by] = parents_and_self\n",
    "        \n",
    "    # TODO: HERE we can sample\n",
    "    keys = list(dict_interpreted_by.keys())\n",
    "    pairs_of_keys = set()\n",
    "    pairs_of_contra_keys = set()\n",
    "    pairs_of_non_contra_keys = set()\n",
    "    without_interpretations = 0\n",
    "    without_interpretations_list = []\n",
    "    for key_i in keys:\n",
    "        for key_j in keys:\n",
    "            if key_i != key_j:\n",
    "                parents_i = set(dict_interpreted_by[key_i])\n",
    "                parents_j = set(dict_interpreted_by[key_j])\n",
    "                descendents_i = key_i.descendant_concepts()\n",
    "                descendents_j = key_j.descendant_concepts()\n",
    "                \n",
    "                parent_intersection = parents_i.intersection(parents_j)\n",
    "                \n",
    "                i_interpretation = set([x.label[0] for x in key_i.has_interpretation])\n",
    "                j_interpretation = set([x.label[0] for x in key_j.has_interpretation])\n",
    "                i_j_interpretation_intersection = i_interpretation.intersection(j_interpretation)\n",
    "                \n",
    "                # not all of them will have an 'interpretation' word\n",
    "                if len(i_interpretation) > 0 and len(j_interpretation) > 0:\n",
    "                    if (key_i,key_j) not in pairs_of_keys and (key_j,key_i) not in pairs_of_keys:\n",
    "                        pairs_of_keys.add((key_i,key_j))\n",
    "                        \n",
    "                        assert len(key_i.label) == 1\n",
    "                        assert len(key_j.label) == 1\n",
    "                        \n",
    "                        predicted_label = determine_label((key_i.label[0],key_j.label[0]),(i_interpretation,j_interpretation))\n",
    "\n",
    "                        if predicted_label==\"1\":\n",
    "                                pairs_of_contra_keys.add((key_i,key_j))\n",
    "                        elif predicted_label==\"0\":\n",
    "                                pairs_of_non_contra_keys.add((key_i,key_j))\n",
    "                        else:\n",
    "                            assert False\n",
    "                else:\n",
    "                    without_interpretations+=1\n",
    "                    without_interpretations_list.append((key_i.name,key_j.name))\n",
    "    \n",
    "    assert pairs_of_keys == pairs_of_contra_keys.union(pairs_of_non_contra_keys)\n",
    "\n",
    "    if sample:\n",
    "        # MAYBE should do random.choice instead if the number is not 0 (to make even number of samples)\n",
    "        if len(pairs_of_keys) < 5:\n",
    "            sampled_keys = pairs_of_keys\n",
    "        elif min(len(pairs_of_contra_keys),len(pairs_of_non_contra_keys)) == 0: \n",
    "            contra_sample = random.sample(pairs_of_contra_keys, min(5,len(pairs_of_contra_keys)))\n",
    "            non_contra_sample = random.sample(pairs_of_non_contra_keys, min(5,len(pairs_of_non_contra_keys)))\n",
    "            sampled_keys = set(contra_sample).union(set(non_contra_sample))\n",
    "            assert len(set(contra_sample).intersection(set(non_contra_sample))) == 0\n",
    "        else:\n",
    "            min_num_samples = min(len(pairs_of_contra_keys), len(pairs_of_non_contra_keys))\n",
    "            contra_sample = random.sample(pairs_of_contra_keys, min_num_samples)\n",
    "            non_contra_sample = random.sample(pairs_of_non_contra_keys, min_num_samples)\n",
    "            sampled_keys = set(contra_sample).union(set(non_contra_sample))\n",
    "            assert len(set(contra_sample).intersection(set(non_contra_sample))) == 0\n",
    "    else:\n",
    "        sampled_keys=pairs_of_keys\n",
    "        \n",
    "    contra_count = 0\n",
    "    non_contra_count = 0\n",
    "    # iterate over all the i_by to see if there is intersection between the parents\n",
    "    # if there is, then they are same side of contra spectrum\n",
    "    for key_i,key_j in sampled_keys:\n",
    "        assert key_i != key_j\n",
    "        parents_i = set(dict_interpreted_by[key_i])\n",
    "        parents_j = set(dict_interpreted_by[key_j])\n",
    "        descendents_i = key_i.descendant_concepts()\n",
    "        descendents_j = key_j.descendant_concepts()\n",
    "\n",
    "        parent_intersection = parents_i.intersection(parents_j)\n",
    "\n",
    "        i_interpretation = set([x.label[0] for x in key_i.has_interpretation])\n",
    "        j_interpretation = set([x.label[0] for x in key_j.has_interpretation])\n",
    "        i_j_interpretation_intersection = i_interpretation.intersection(j_interpretation)\n",
    "        \n",
    "        assert len(i_interpretation) > 0 and len(j_interpretation) > 0\n",
    "        # in theory: dont contradict?\n",
    "        # if len(parent_intersection) == 0:\n",
    "        assert len(key_i.label) == 1\n",
    "        assert len(key_j.label) == 1\n",
    "        count+=1\n",
    "\n",
    "        i_cui_ids = tuple([x.name for x in SNOMEDCT_US[key_i.name].unifieds])\n",
    "        j_cui_ids = tuple([x.name for x in SNOMEDCT_US[key_j.name].unifieds])\n",
    "\n",
    "        key_i_label = f\"{str(i_interpretation)}: {key_i.label[0]}|{i_cui_ids}|{key_i.name}\"\n",
    "        key_j_label = f\"{str(j_interpretation)}: {key_j.label[0]}|{j_cui_ids}|{key_j.name}\"\n",
    "        assert len(i_cui_ids) > 0\n",
    "        assert len(j_cui_ids) > 0\n",
    "\n",
    "        predicted_label = determine_label((key_i.label[0],key_j.label[0]),(i_interpretation,j_interpretation))\n",
    "\n",
    "        if predicted_label == \"1\":\n",
    "            assert (key_i,key_j) in pairs_of_contra_keys or (key_j,key_i) in pairs_of_contra_keys\n",
    "\n",
    "            if (key_i_label,key_j_label) not in phrase_dict[\"contra\"] and \\\n",
    "            (key_j_label,key_i_label) not in phrase_dict[\"contra\"]:\n",
    "                phrase_dict[\"contra\"].add((key_i_label,key_j_label))\n",
    "                contra_count+=1\n",
    "                \n",
    "            for i_int in i_interpretation:\n",
    "                for j_int in j_interpretation:\n",
    "                    pair1 = (i_int,j_int)\n",
    "                    pair2 = (j_int,i_int)\n",
    "\n",
    "                    if pair1 not in single_word_dict[\"contra\"] and pair2 not in single_word_dict[\"contra\"]:\n",
    "                        single_word_dict[\"contra\"].add(pair1)                \n",
    "        elif predicted_label == \"0\":\n",
    "            assert (key_i,key_j) in pairs_of_non_contra_keys or (key_j,key_i) in pairs_of_non_contra_keys\n",
    "\n",
    "\n",
    "            # sometimes the pairs are already populated by means of getting there through a different\n",
    "            # ancestor\n",
    "            if (key_i_label,key_j_label) not in phrase_dict[\"non-contra\"] and \\\n",
    "            (key_j_label,key_i_label) not in phrase_dict[\"non-contra\"]:\n",
    "                phrase_dict[\"non-contra\"].add((key_i_label,key_j_label))\n",
    "                non_contra_count+=1\n",
    "                \n",
    "            for i_int in i_interpretation:\n",
    "                for j_int in j_interpretation:\n",
    "                    pair1 = (i_int,j_int)\n",
    "                    pair2 = (j_int,i_int)\n",
    "\n",
    "                    if pair1 not in single_word_dict[\"non-contra\"] and pair2 not in single_word_dict[\"non-contra\"]:\n",
    "                        single_word_dict[\"non-contra\"].add(pair1) \n",
    "                        \n",
    "        else:\n",
    "            assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edcdb8-249f-4ddd-975b-321faf841823",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_with_interpretations = 0\n",
    "set_of_has_interpretations = set()\n",
    "single_word_dict = {\"contra\":set(),\"non-contra\":set()}\n",
    "phrase_dict = {\"contra\":set(),\"non-contra\":set()}\n",
    "\n",
    "num_interpretations= []\n",
    "inter_parent_to_sid = {}\n",
    "\n",
    "num_children = 0\n",
    "relev_children = []\n",
    "\n",
    "# can iteratate over subfields as well...\n",
    "for child in all_nodes:\n",
    "    num_children+=1\n",
    "    child_concept = SNOMEDCT_US[child]\n",
    "    interpreted_by = child_concept.is_interpreted_by\n",
    "    inter_parent_to_sid[child_concept.label[0]] = child\n",
    "        \n",
    "    if interpreted_by != []:\n",
    "        num_interpretations.append((len(interpreted_by),child_concept.label[0]))\n",
    "\n",
    "    # can also filter by group size here, by adding additional conditional.  \n",
    "    # len(interpreted_by) is the group size\n",
    "    if interpreted_by != [] and len(interpreted_by) < GROUP_SIZE:\n",
    "        relev_children.append(child_concept.label[0])\n",
    "        determine_interpretation_relationship(interpreted_by, single_word_dict, phrase_dict,sample=False)\n",
    "        for i_by in interpreted_by:\n",
    "            assert len(i_by.label) == 1\n",
    "            has_interpretations = [x.label[0] for x in i_by.has_interpretation]\n",
    "            for item in has_interpretations: set_of_has_interpretations.add(item)\n",
    "            if len(has_interpretations) > 0:\n",
    "                elements_with_interpretations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37d8bb-d71a-436d-a2d1-92214a999edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all phrases to a set\n",
    "set_of_phrases = set()\n",
    "set_of_cuis = set()\n",
    "total = 0\n",
    "for key in phrase_dict:\n",
    "    for elem1,elem2 in phrase_dict[key]:\n",
    "        words1, phrase1_cuid = [x.strip() for x in elem1.split(\":\",1)]\n",
    "        words2, phrase2_cuid = [x.strip() for x in elem2.split(\":\",1)]\n",
    "\n",
    "        phrase1,cuid1,snomed_ui1 = [x.strip() for x in phrase1_cuid.split(\"|\")]\n",
    "        phrase2,cuid2,snomed_ui2 = [x.strip() for x in phrase2_cuid.split(\"|\")]\n",
    "        cuid1=eval(cuid1)\n",
    "        cuid2=eval(cuid2)\n",
    "        \n",
    "        cuid1_labels = []\n",
    "        cuid2_labels = []\n",
    "        \n",
    "        for cuid in cuid1:\n",
    "            cuid1_labels.append(CUI[cuid].label)\n",
    "            \n",
    "        for cuid in cuid2:\n",
    "            cuid2_labels.append(CUI[cuid].label)\n",
    "            \n",
    "        phrase1_cuid += f\"|{cuid1_labels}\"\n",
    "        phrase2_cuid += f\"|{cuid2_labels}\"\n",
    "        \n",
    "        assert len(cuid1) == len(cuid1_labels)\n",
    "        \n",
    "        phrase1_text = f\"{phrase1}\\t{cuid1}\\t{cuid1_labels}\\t{snomed_ui1}\"\n",
    "        phrase2_text = f\"{phrase2}\\t{cuid2}\\t{cuid2_labels}\\t{snomed_ui2}\"\n",
    "        set_of_phrases.add(phrase1_text)\n",
    "        set_of_phrases.add(phrase2_text)\n",
    "        set_of_cuis.add(phrase1_cuid)\n",
    "        set_of_cuis.add(phrase2_cuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca506c-c77b-472e-913f-78f4697e4330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write phrases to a text file\n",
    "with open(\"snomed_phrases.tsv\",\"w\") as f:    \n",
    "    f.write(f\"phrase\\tcuis\\tcui_terms\\n\")\n",
    "    f.writelines([x + \"\\n\" for x in set_of_phrases])\n",
    "with open(\"snomed_phrases.txt\",\"w\") as f:    \n",
    "    f.writelines([x.split(\"\\t\")[0].strip() + \"\\n\" for x in set_of_phrases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b911c-112c-4cd2-b9b0-2334ec733e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_list = []\n",
    "shuffled_list += list([(x,\"0\") for x in phrase_dict[\"non-contra\"]])\n",
    "shuffled_list += list([(x,\"1\") for x in phrase_dict[\"contra\"]])\n",
    "\n",
    "random.shuffle(shuffled_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13951972-39e4-4126-9b5c-2632ddc552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phrase_pairs = set()\n",
    "num_in_plia_dict = 0\n",
    "mislabeled = 0\n",
    "with open(\"snomed_pairs_auto_labeled.csv\", \"w\") as f:\n",
    "    f.write(\",\".join([\"A: Word(s)\",\"A: Phrase\", \"B: Word(s)\", \"B: Phrase\", \"Auto Label\",\"\\n\"]))\n",
    "    for (elem1,elem2),auto_label in shuffled_list:\n",
    "        words1, phrase1 = [x.strip() for x in elem1.split(\":\",1)]\n",
    "        words2, phrase2 = [x.strip() for x in elem2.split(\":\",1)]\n",
    "        phrase1 = phrase1.split(\"|\")[0]\n",
    "        phrase2 = phrase2.split(\"|\")[0]\n",
    "        \n",
    "        all_phrase_pairs.add((phrase1,phrase2))\n",
    "        all_phrase_pairs.add((phrase2,phrase1))\n",
    "        \n",
    "        phrase1 = phrase1.replace(',', '')\n",
    "        phrase2 = phrase2.replace(',', '')\n",
    "        \n",
    "        assert len(eval(words1)) >= 1\n",
    "        assert len(eval(words2)) >= 1\n",
    "        \n",
    "        words1 = \" | \".join(eval(words1))\n",
    "        words2 = \" | \".join(eval(words2))\n",
    "        \n",
    "        f.write(\",\".join([words1,phrase1,words2,phrase2,str(auto_label)])+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
